{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMU6HpYmv75OcxVqHCpbmU8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Programmation dynamique à horizon infini"],"metadata":{"id":"8zTX99imGXqj"}},{"cell_type":"code","source":["\n","# Question 1\n","\n","graph = {\n","    0: [(1, 1, 1), (2, 5, 1), (3, -3, 1)],\n","    1: [(4, -2, 1), (5, 5, 1)],\n","    2: [(4, 2, 1), (5, 7, 1), (6, 4, 1)],\n","    3: [(5, 7, 1), (6, 3, 1)],\n","    4: [(7, 3, 1), (8, 5, 1)],\n","    5: [(7, -2, 1), (8, 1, 1), (9, 2, 1)],\n","    6: [(8, 0, 1), (9, 4, 1)],\n","    7: [(0, 0, 0), (0, 0, 0)],\n","    8: [(0, 0, 0), (0, 0, 0)],\n","    9: [(0, 0, 0), (0, 0, 0)]\n","}\n","graph\n"],"metadata":{"id":"W6Wy7QIIGUjo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679750257944,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jordan Foute","userId":"01040215620773108624"}},"outputId":"d82c9e1a-5de5-4d19-d11d-a79b7e1f5564"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [(1, 1, 1), (2, 5, 1), (3, -3, 1)],\n"," 1: [(4, -2, 1), (5, 5, 1)],\n"," 2: [(4, 2, 1), (5, 7, 1), (6, 4, 1)],\n"," 3: [(5, 7, 1), (6, 3, 1)],\n"," 4: [(7, 3, 1), (8, 5, 1)],\n"," 5: [(7, -2, 1), (8, 1, 1), (9, 2, 1)],\n"," 6: [(8, 0, 1), (9, 4, 1)],\n"," 7: [(0, 0, 0), (0, 0, 0)],\n"," 8: [(0, 0, 0), (0, 0, 0)],\n"," 9: [(0, 0, 0), (0, 0, 0)]}"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Détermination de la valeur des états par récursivité inverse\n","\n","max_t = 3\n","def mdp_value_function(graph, max_t):\n","    # Initialisation de la fonction de valeur pour chaque état du graphe\n","    value_function = {state: 0 for state in graph.keys()}\n","    #print(value_function)\n","    # Parcours des niveaux de l'arbre de décision de bas en haut\n","    for t in range(max_t, -1, -1):\n","        # Calcul de la fonction de valeur pour chaque état du graphe\n","        for state in graph.keys():\n","            if t == max_t:\n","                # La fonction de valeur pour les états terminaux est nulle\n","                value_function[state] = 0\n","            else:\n","                # Calcul de la valeur pour chaque action possible\n","                action_values = []\n","                for next_state, cost, probability in graph[state]:\n","                    next_value = value_function[next_state]\n","                    action_values.append(probability * (cost + next_value))\n","                # La valeur pour l'état courant est la meilleure valeur parmi les actions possibles\n","                \n","                value_function[state] = max(action_values)\n","    \n","    # Retourne la fonction de valeur pour chaque état du graphe\n","    return value_function"],"metadata":{"id":"EPdgKDYxtSyc","executionInfo":{"status":"ok","timestamp":1679750263015,"user_tz":-60,"elapsed":6,"user":{"displayName":"Jordan Foute","userId":"01040215620773108624"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(mdp_value_function(graph,max_t))\n","value_function = mdp_value_function(graph,max_t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjuP8XSKtUYM","executionInfo":{"status":"ok","timestamp":1679750269027,"user_tz":-60,"elapsed":10,"user":{"displayName":"Jordan Foute","userId":"01040215620773108624"}},"outputId":"3afb8d3e-d09c-4b88-e938-fe7753a475eb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 14, 1: 7, 2: 9, 3: 9, 4: 5, 5: 2, 6: 4, 7: 0, 8: 0, 9: 0}\n"]}]},{"cell_type":"code","source":["# Calcul de la politique optimale, le chemin le plus long\n","\n","# Notre fonction prend en parametre la value function de chaque etat\n","def find_optimal_path(graph, value_function):\n","  #  initialisation de l'état courant à l'état initial\n","    current_state = 0  # on commence à l'état initial\n","    path = [current_state]\n","    for k in range(0,max_t):\n","        best_action = None\n","        best_action_value = -float('inf')\n","        for next_state, cost, probability in graph[current_state]:\n","            next_value = value_function[next_state]\n","            action_value = probability * (cost + next_value)\n","            if action_value > best_action_value:\n","                best_action = next_state\n","                best_action_value = action_value\n","        # vérification si l'état courant est l'état terminal\n","        if best_action is None:  # on est arrivé à l'état terminal\n","            break\n","        path.append(best_action)\n","\n","        #affectation de la meilleure action à l'état courant\n","        current_state = best_action  # on passe à l'état suivant\n","    return path\n","print(find_optimal_path(graph, value_function))"],"metadata":{"id":"4lnTqgGXHkle","executionInfo":{"status":"ok","timestamp":1679750272064,"user_tz":-60,"elapsed":233,"user":{"displayName":"Jordan Foute","userId":"01040215620773108624"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3873db8a-f8a6-4e2a-f239-5c7e1b4c940d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 2, 5, 9]\n"]}]},{"cell_type":"markdown","source":["# Programmation dynamique à horizon infini"],"metadata":{"id":"ehnvbLpfGIMS"}},{"cell_type":"code","source":["# 1) Iteration de la valeur \n","\n","import numpy as np\n","\n","# Définir les paramètres du MDP\n","n = 3\n","m = 4\n","# etat initiale\n","start_state = (0, 0)\n","\n","# Matrices de Recompenses\n","rewards = np.zeros((n, m))\n","\n","rewards[1, 3] = -1\n","rewards[0, 3] = 1\n","\n","gamma = 0.9\n","\n","\n","# pour verifier la condition d'arrete\n","epsilon = 0.01\n","\n","# Définir les probabilités de transition pour chaque action et couple d'états\n","\n","def get_transition_probs(state, action):\n","    transitions = []\n","    row, col = state\n","\n","    if action == 'up':\n","        next_state = (row - 1, col)\n","        if next_state[0] >= 0:\n","            transitions.append((0.8, next_state))\n","        if col > 0:\n","            transitions.append((0.1, (row, col - 1)))\n","        if col < m - 1:\n","            transitions.append((0.1, (row, col + 1)))\n","    elif action == 'down':\n","        next_state = (row + 1, col)\n","        if next_state[0] < n:\n","            transitions.append((0.8, next_state))\n","        if col > 0:\n","            transitions.append((0.1, (row, col - 1)))\n","        if col < m - 1:\n","            transitions.append((0.1, (row, col + 1)))\n","    elif action == 'left':\n","        next_state = (row, col - 1)\n","        if next_state[1] >= 0:\n","            transitions.append((0.8, next_state))\n","        if row > 0:\n","            transitions.append((0.1, (row - 1, col)))\n","        if row < n - 1:\n","            transitions.append((0.1, (row + 1, col)))\n","    elif action == 'right':\n","        next_state = (row, col + 1)\n","        if next_state[1] < m:\n","            transitions.append((0.8, next_state))\n","        if row > 0:\n","            transitions.append((0.1, (row - 1, col)))\n","        if row < n - 1:\n","            transitions.append((0.1, (row + 1, col)))\n","    else:\n","        raise ValueError('Invalid action')\n","\n","    return transitions\n","\n","# Initialiser les valeurs de tous les états à 0\n","values = np.zeros((n, m))\n","\n","# Appliquer l'algorithme de programmation dynamique par itération de la valeur\n","while True:\n","  # Cette variable est utilisée pour suivre la différence maximale entre les nouvelles valeurs et les anciennes valeurs dans la matrice de valeurs à chaque itération de la boucle.\n","    delta = 0\n","    for i in range(n):\n","        for j in range(m):\n","            state = (i, j)\n","            max_value = float('-inf')\n","            # Etant dans un etat i,j on a un ensemble d'acion qu'on peut effectuer\n","            for action in ['up', 'down', 'left', 'right']:\n","\n","                \n","                transitions = get_transition_probs(state, action)\n","                action_value = 0\n","                for prob, next_state in transitions:\n","                  # on accède aux recommpense dans la grille a partir de la valeur de l'etat suivant\n","                    next_value = values[next_state[0], next_state[1]]\n","                    action_value += prob * (rewards[next_state[0], next_state[1]] + gamma * next_value)\n","                if action_value > max_value:\n","                    max_value = action_value\n","            delta = max(delta, abs(values[i, j] - max_value))\n","            values[i, j] = max_value\n","    if delta < epsilon:\n","        break\n","\n","# Afficher les valeurs de tous les états\n","print(values)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgC1Ffq4istE","executionInfo":{"status":"ok","timestamp":1679750277958,"user_tz":-60,"elapsed":9,"user":{"displayName":"Jordan Foute","userId":"01040215620773108624"}},"outputId":"acaa8bd4-77cf-4a05-f393-196af2c47d49"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.30447669 1.65641187 2.116166   1.61409446]\n"," [1.30628174 1.52539982 1.75138045 2.11977225]\n"," [1.06900002 1.32768543 1.4845216  1.15963505]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vt9HzTWokjU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2) iteration de la politique\n","\n","# policy = np.random.choice(['up', 'down', 'left', 'right'], size=(n, m))\n","# policy\n","\n","# Définir la fonction d'itération de politique ( Amelioration de la politique)\n","def improve_policy(policy, values):\n","    policy_stable = True\n","    for i in range(n):\n","        for j in range(m):\n","            state = (i, j)\n","            old_action = policy[state]\n","            best_action = None\n","            best_value = float('-inf')\n","            for action in ['up', 'down', 'left', 'right']:\n","                transitions = get_transition_probs(state, action)\n","                action_value = 0\n","                for prob, next_state in transitions:\n","                    next_value = values[next_state[0], next_state[1]]\n","                    action_value += prob * (rewards[next_state[0], next_state[1]] + gamma * next_value)\n","                if action_value > best_value:\n","                    best_value = action_value\n","                    best_action = action\n","            policy[state] = best_action\n","            if old_action != best_action:\n","                policy_stable = False\n","    return policy_stable\n","\n","\n","#algorithme d'itération de la valeur pour obtenir les valeurs de chaque état pour une politique donnée\n","def compute_state_values(policy,values, rewards, gamma, epsilon):\n","    n, m = rewards.shape\n","\n","    # Initialiser les valeurs de tous les états à 0\n","  \n","\n","    while True:\n","        delta = 0\n","\n","        # Pour chaque état, calculer sa nouvelle valeur\n","        for i in range(n):\n","            for j in range(m):\n","                state = (i, j)\n","                old_value = values[state]\n","\n","                # Calculer la nouvelle valeur en appliquant la politique courante\n","                new_value = 0\n","                for prob, next_state in get_transition_probs(state, policy[state]):\n","                    new_value += prob * (rewards[next_state[0], next_state[1]] + gamma * values[next_state[0], next_state[1]])\n","\n","                # Mettre à jour la valeur de l'état courant\n","                values[state] = new_value\n","\n","                # Calculer la différence entre les anciennes et nouvelles valeurs\n","                delta = max(delta, abs(old_value - new_value))\n","\n","        # Si la convergence est atteinte, sortir de la boucle\n","        if delta < epsilon:\n","            break\n","\n","    return values\n","\n","# Initialiser les valeurs de tous les états à 0\n","values = np.zeros((n, m))\n","\n","# Initialiser la politique avec une action aléatoire pour chaque état\n","policy = np.random.choice(['up', 'down', 'left', 'right'], size=(n, m)\n",")\n","# Appliquer l'algorithme d'itération de politique\n","policy_stable = False\n","i = 0\n","while not policy_stable:\n","    i = i + 1\n","    # Appliquer l'algorithme d'itération de la valeur pour obtenir les valeurs de chaque état\n","    values = compute_state_values(policy,values,rewards,gamma, epsilon)\n","\n","    # Améliorer la politique\n","    print(\"iteration N° \", i)\n","    print(policy)\n","    policy_stable = improve_policy(policy, values)\n","\n","# Afficher la politique finale\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZTNe5Oh4NA2","executionInfo":{"status":"ok","timestamp":1679752586206,"user_tz":-60,"elapsed":262,"user":{"displayName":"Jordan Foute","userId":"01040215620773108624"}},"outputId":"6aa4b114-01ea-40f5-a23a-93a8a4ad6f72"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["iteration N°  1\n","[['down' 'down' 'down' 'down']\n"," ['up' 'left' 'down' 'up']\n"," ['up' 'up' 'up' 'down']]\n","iteration N°  2\n","[['left' 'left' 'right' 'up']\n"," ['left' 'left' 'left' 'up']\n"," ['left' 'left' 'down' 'down']]\n","iteration N°  3\n","[['left' 'right' 'right' 'left']\n"," ['left' 'right' 'up' 'up']\n"," ['left' 'down' 'up' 'down']]\n","iteration N°  4\n","[['right' 'right' 'right' 'left']\n"," ['right' 'right' 'up' 'up']\n"," ['right' 'up' 'up' 'left']]\n"]}]}]}